{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probabilistic Matrix Factorization for Making Personalized Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pymc3 as pm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.style.use('seaborn-darkgrid')\n",
    "print('Running on PyMC3 v{}'.format(pm.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(pm.get_data(\"ml_100k_u.data\"), \n",
    "                   sep='\\t', names=[\"userid\", \"itemid\", \"rating\", \"timestamp\"])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_columns  = ['movie id', 'movie title', 'release date', 'video release date', 'IMDb URL',\n",
    "                  'unknown','Action','Adventure', 'Animation',\"Children's\", 'Comedy', 'Crime',\n",
    "                  'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror', 'Musical', 'Mystery',\n",
    "                  'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western']\n",
    "movies = pd.read_csv(pm.get_data(\"ml_100k_u.item\"), sep=\"|\", names=movie_columns, index_col=\"movie id\", parse_dates=['release date'])\n",
    "movies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the ratings from the DataFrame\n",
    "ratings = data.rating\n",
    "\n",
    "# Plot histogram\n",
    "data.groupby('rating').size().plot(kind='bar');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.rating.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_means = data.join(movies['movie title'], on='itemid').groupby('movie title').rating.mean()\n",
    "movie_means[:50].plot(kind='bar', grid=False, figsize=(16, 6),\n",
    "                      title=\"Mean ratings for 50 movies\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(16, 4), sharey=True)\n",
    "movie_means.nlargest(30).plot(kind='bar', ax=ax1,title=\"Top 30 movies in data set\");\n",
    "movie_means.nsmallest(30).plot(kind='bar', ax=ax2,title=\"Bottom 30 movies in data set\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_means = data.groupby('userid').rating.mean().sort_values()\n",
    "_, ax = plt.subplots(figsize=(16, 6))\n",
    "ax.plot(np.arange(len(user_means)), user_means.values, 'k-')\n",
    "\n",
    "ax.fill_between(np.arange(len(user_means)), user_means.values, alpha=0.3)\n",
    "ax.set_xticklabels('');  # 1000 labels is nonsensical\n",
    "ax.set_ylabel('Rating')\n",
    "ax.set_xlabel(f'{len(user_means)} average ratings per user')\n",
    "ax.set_ylim(0, 5)\n",
    "ax.set_xlim(0, len(user_means));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a base class with scaffolding for our 3 baselines.\n",
    "\n",
    "def split_title(title):\n",
    "    \"\"\"Change \"BaselineMethod\" to \"Baseline Method\".\"\"\"\n",
    "    words = []\n",
    "    tmp = [title[0]]\n",
    "    for c in title[1:]:\n",
    "        if c.isupper():\n",
    "            words.append(''.join(tmp))\n",
    "            tmp = [c]\n",
    "        else:\n",
    "            tmp.append(c)\n",
    "    words.append(''.join(tmp))\n",
    "    return ' '.join(words)\n",
    "\n",
    "\n",
    "class Baseline():\n",
    "    \"\"\"Calculate baseline predictions.\"\"\"\n",
    "\n",
    "    def __init__(self, train_data):\n",
    "        \"\"\"Simple heuristic-based transductive learning to fill in missing\n",
    "        values in data matrix.\"\"\"\n",
    "        self.predict(train_data.copy())\n",
    "\n",
    "    def predict(self, train_data):\n",
    "        raise NotImplementedError(\n",
    "            'baseline prediction not implemented for base class')\n",
    "\n",
    "    def rmse(self, test_data):\n",
    "        \"\"\"Calculate root mean squared error for predictions on test data.\"\"\"\n",
    "        return rmse(test_data, self.predicted)\n",
    "\n",
    "    def __str__(self):\n",
    "        return split_title(self.__class__.__name__)\n",
    "\n",
    "\n",
    "# Implement the 3 baselines.\n",
    "\n",
    "class UniformRandomBaseline(Baseline):\n",
    "    \"\"\"Fill missing values with uniform random values.\"\"\"\n",
    "\n",
    "    def predict(self, train_data):\n",
    "        nan_mask = np.isnan(train_data)\n",
    "        masked_train = np.ma.masked_array(train_data, nan_mask)\n",
    "        pmin, pmax = masked_train.min(), masked_train.max()\n",
    "        N = nan_mask.sum()\n",
    "        train_data[nan_mask] = np.random.uniform(pmin, pmax, N)\n",
    "        self.predicted = train_data\n",
    "\n",
    "\n",
    "class GlobalMeanBaseline(Baseline):\n",
    "    \"\"\"Fill in missing values using the global mean.\"\"\"\n",
    "\n",
    "    def predict(self, train_data):\n",
    "        nan_mask = np.isnan(train_data)\n",
    "        train_data[nan_mask] = train_data[~nan_mask].mean()\n",
    "        self.predicted = train_data\n",
    "\n",
    "\n",
    "class MeanOfMeansBaseline(Baseline):\n",
    "    \"\"\"Fill in missing values using mean of user/item/global means.\"\"\"\n",
    "\n",
    "    def predict(self, train_data):\n",
    "        nan_mask = np.isnan(train_data)\n",
    "        masked_train = np.ma.masked_array(train_data, nan_mask)\n",
    "        global_mean = masked_train.mean()\n",
    "        user_means = masked_train.mean(axis=1)\n",
    "        item_means = masked_train.mean(axis=0)\n",
    "        self.predicted = train_data.copy()\n",
    "        n, m = train_data.shape\n",
    "        for i in range(n):\n",
    "            for j in range(m):\n",
    "                if np.ma.isMA(item_means[j]):\n",
    "                    self.predicted[i, j] = np.mean(\n",
    "                        (global_mean, user_means[i]))\n",
    "                else:\n",
    "                    self.predicted[i, j] = np.mean(\n",
    "                        (global_mean, user_means[i], item_means[j]))\n",
    "\n",
    "\n",
    "baseline_methods = {}\n",
    "baseline_methods['ur'] = UniformRandomBaseline\n",
    "baseline_methods['gm'] = GlobalMeanBaseline\n",
    "baseline_methods['mom'] = MeanOfMeansBaseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_users = data.userid.unique().shape[0]\n",
    "num_items = data.itemid.unique().shape[0]\n",
    "sparsity = 1 - len(data) / (num_users * num_items)\n",
    "print(f\"Users: {num_users}\\nMovies: {num_items}\\nSparsity: {sparsity}\")\n",
    "\n",
    "dense_data = data.pivot(index = 'userid', columns ='itemid', values = 'rating').values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import logging\n",
    "import theano\n",
    "import scipy as sp\n",
    "\n",
    "\n",
    "# Enable on-the-fly graph computations, but ignore\n",
    "# absence of intermediate test values.\n",
    "theano.config.compute_test_value = 'ignore'\n",
    "\n",
    "# Set up logging.\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "\n",
    "class PMF():\n",
    "    \"\"\"Probabilistic Matrix Factorization model using pymc3.\"\"\"\n",
    "\n",
    "    def __init__(self, train, dim, alpha=2, std=0.01, bounds=(1, 5)):\n",
    "        \"\"\"Build the Probabilistic Matrix Factorization model using pymc3.\n",
    "\n",
    "        :param np.ndarray train: The training data to use for learning the model.\n",
    "        :param int dim: Dimensionality of the model; number of latent factors.\n",
    "        :param int alpha: Fixed precision for the likelihood function.\n",
    "        :param float std: Amount of noise to use for model initialization.\n",
    "        :param (tuple of int) bounds: (lower, upper) bound of ratings.\n",
    "            These bounds will simply be used to cap the estimates produced for R.\n",
    "\n",
    "        \"\"\"\n",
    "        self.dim = dim\n",
    "        self.alpha = alpha\n",
    "        self.std = np.sqrt(1.0 / alpha)\n",
    "        self.bounds = bounds\n",
    "        self.data = train.copy()\n",
    "        n, m = self.data.shape\n",
    "\n",
    "        # Perform mean value imputation\n",
    "        nan_mask = np.isnan(self.data)\n",
    "        self.data[nan_mask] = self.data[~nan_mask].mean()\n",
    "\n",
    "        # Low precision reflects uncertainty; prevents overfitting.\n",
    "        # Set to the mean variance across users and items.\n",
    "        self.alpha_u = 1 / self.data.var(axis=1).mean()\n",
    "        self.alpha_v = 1 / self.data.var(axis=0).mean()\n",
    "\n",
    "        # Specify the model.\n",
    "        logging.info('building the PMF model')\n",
    "        with pm.Model() as pmf:\n",
    "            U = pm.MvNormal(\n",
    "                'U', mu=0, tau=self.alpha_u * np.eye(dim),\n",
    "                shape=(n, dim), testval=np.random.randn(n, dim) * std)\n",
    "            V = pm.MvNormal(\n",
    "                'V', mu=0, tau=self.alpha_v * np.eye(dim),\n",
    "                shape=(m, dim), testval=np.random.randn(m, dim) * std)\n",
    "            R = pm.Normal(\n",
    "                'R', mu=(U @ V.T)[~nan_mask], tau=self.alpha,\n",
    "                observed=self.data[~nan_mask])\n",
    "\n",
    "        logging.info('done building the PMF model')\n",
    "        self.model = pmf\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _find_map(self):\n",
    "    \"\"\"Find mode of posterior using L-BFGS-B optimization.\"\"\"\n",
    "    tstart = time.time()\n",
    "    with self.model:\n",
    "        logging.info('finding PMF MAP using L-BFGS-B optimization...')\n",
    "        self._map = pm.find_MAP(method='L-BFGS-B')\n",
    "\n",
    "    elapsed = int(time.time() - tstart)\n",
    "    logging.info('found PMF MAP in %d seconds' % elapsed)\n",
    "    return self._map\n",
    "\n",
    "def _map(self):\n",
    "    try:\n",
    "        return self._map\n",
    "    except:\n",
    "        return self.find_map()\n",
    "\n",
    "# Update our class with the new MAP infrastructure.\n",
    "PMF.find_map = _find_map\n",
    "PMF.map = property(_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw MCMC samples.\n",
    "def _draw_samples(self, **kwargs):\n",
    "    kwargs.setdefault('chains', 1)\n",
    "    with self.model:\n",
    "        self.trace = pm.sample(**kwargs)\n",
    "\n",
    "# Update our class with the sampling infrastructure.\n",
    "PMF.draw_samples = _draw_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _predict(self, U, V):\n",
    "    \"\"\"Estimate R from the given values of U and V.\"\"\"\n",
    "    R = np.dot(U, V.T)\n",
    "    n, m = R.shape\n",
    "    sample_R = np.random.normal(R, self.std)\n",
    "    # bound ratings\n",
    "    low, high = self.bounds\n",
    "    sample_R[sample_R < low] = low\n",
    "    sample_R[sample_R > high] = high\n",
    "    return sample_R\n",
    "\n",
    "\n",
    "PMF.predict = _predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our evaluation function.\n",
    "def rmse(test_data, predicted):\n",
    "    \"\"\"Calculate root mean squared error.\n",
    "    Ignoring missing values in the test data.\n",
    "    \"\"\"\n",
    "    I = ~np.isnan(test_data)   # indicator for missing values\n",
    "    N = I.sum()                # number of non-missing values\n",
    "    sqerror = abs(test_data - predicted) ** 2  # squared error array\n",
    "    mse = sqerror[I].sum() / N                 # mean squared error\n",
    "    return np.sqrt(mse)                        # RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function for splitting train/test data.\n",
    "def split_train_test(data, percent_test=0.1):\n",
    "    \"\"\"Split the data into train/test sets.\n",
    "    :param int percent_test: Percentage of data to use for testing. Default 10.\n",
    "    \"\"\"\n",
    "    n, m = data.shape             # # users, # movies\n",
    "    N = n * m                     # # cells in matrix\n",
    "\n",
    "    # Prepare train/test ndarrays.\n",
    "    train = data.copy()\n",
    "    test = np.ones(data.shape) * np.nan\n",
    "\n",
    "    # Draw random sample of training data to use for testing.\n",
    "    tosample = np.where(~np.isnan(train))       # ignore nan values in data\n",
    "    idx_pairs = list(zip(tosample[0], tosample[1]))   # tuples of row/col index pairs\n",
    "\n",
    "    test_size = int(len(idx_pairs) * percent_test)  # use 10% of data as test set\n",
    "    train_size = len(idx_pairs) - test_size   # and remainder for training\n",
    "\n",
    "    indices = np.arange(len(idx_pairs))         # indices of index pairs\n",
    "    sample = np.random.choice(indices, replace=False, size=test_size)\n",
    "\n",
    "    # Transfer random sample from train set to test set.\n",
    "    for idx in sample:\n",
    "        idx_pair = idx_pairs[idx]\n",
    "        test[idx_pair] = train[idx_pair]  # transfer to test set\n",
    "        train[idx_pair] = np.nan          # remove from train set\n",
    "\n",
    "    # Verify everything worked properly\n",
    "    assert(train_size == N-np.isnan(train).sum())\n",
    "    assert(test_size == N-np.isnan(test).sum())\n",
    "\n",
    "    # Return train set and test set\n",
    "    return train, test\n",
    "\n",
    "train, test = split_train_test(dense_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see the results:\n",
    "baselines = {}\n",
    "for name in baseline_methods:\n",
    "    Method = baseline_methods[name]\n",
    "    method = Method(train)\n",
    "    baselines[name] = method.rmse(test)\n",
    "    print('%s RMSE:\\t%.5f' % (method, baselines[name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use a fixed precision for the likelihood.\n",
    "# This reflects uncertainty in the dot product.\n",
    "# We choose 2 in the footsteps Salakhutdinov\n",
    "# Mnihof.\n",
    "ALPHA = 2\n",
    "\n",
    "# The dimensionality D; the number of latent factors.\n",
    "# We can adjust this higher to try to capture more subtle\n",
    "# characteristics of each movie. However, the higher it is,\n",
    "# the more expensive our inference procedures will be.\n",
    "# Specifically, we have D(N + M) latent variables. For our\n",
    "# Movielens dataset, this means we have D(2625), so for 5\n",
    "# dimensions, we are sampling 13125 latent variables.\n",
    "DIM = 10\n",
    "\n",
    "\n",
    "pmf = PMF(train, DIM, ALPHA, std=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find MAP for PMF.\n",
    "pmf.find_map();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_map(pmf_model, train, test):\n",
    "    U = pmf_model.map['U']\n",
    "    V = pmf_model.map['V']\n",
    "\n",
    "    # Make predictions and calculate RMSE on train & test sets.\n",
    "    predictions = pmf_model.predict(U, V)\n",
    "    train_rmse = rmse(train, predictions)\n",
    "    test_rmse = rmse(test, predictions)\n",
    "    overfit = test_rmse - train_rmse\n",
    "\n",
    "    # Print report.\n",
    "    print('PMF MAP training RMSE: %.5f' % train_rmse)\n",
    "    print('PMF MAP testing RMSE:  %.5f' % test_rmse)\n",
    "    print('Train/test difference: %.5f' % overfit)\n",
    "\n",
    "    return test_rmse\n",
    "\n",
    "\n",
    "# Add eval function to PMF class.\n",
    "PMF.eval_map = eval_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate PMF MAP estimates.\n",
    "pmf_map_rmse = pmf.eval_map(train, test)\n",
    "pmf_improvement = baselines['mom'] - pmf_map_rmse\n",
    "print('PMF MAP Improvement:   %.5f' % pmf_improvement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw MCMC samples.\n",
    "pmf.draw_samples(draws=500, tune=500, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _norms(pmf_model, monitor=('U', 'V'), ord='fro'):\n",
    "    \"\"\"Return norms of latent variables at each step in the\n",
    "    sample trace. These can be used to monitor convergence\n",
    "    of the sampler.\n",
    "    \"\"\"\n",
    "    monitor = ('U', 'V')\n",
    "    norms = {var: [] for var in monitor}\n",
    "    for sample in pmf_model.trace:\n",
    "        for var in monitor:\n",
    "            norms[var].append(np.linalg.norm(sample[var], ord))\n",
    "    return norms\n",
    "\n",
    "\n",
    "def _traceplot(pmf_model):\n",
    "    \"\"\"Plot Frobenius norms of U and V as a function of sample #.\"\"\"\n",
    "    trace_norms = pmf_model.norms()\n",
    "    u_series = pd.Series(trace_norms['U'])\n",
    "    v_series = pd.Series(trace_norms['V'])\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 7))\n",
    "    u_series.plot(kind='line', ax=ax1, grid=False,\n",
    "                  title=\"$\\|U\\|_{Fro}^2$ at Each Sample\")\n",
    "    v_series.plot(kind='line', ax=ax2, grid=False,\n",
    "                  title=\"$\\|V\\|_{Fro}^2$ at Each Sample\")\n",
    "    ax1.set_xlabel(\"Sample Number\")\n",
    "    ax2.set_xlabel(\"Sample Number\")\n",
    "\n",
    "\n",
    "PMF.norms = _norms\n",
    "PMF.traceplot = _traceplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pmf.traceplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _running_rmse(pmf_model, test_data, train_data, burn_in=0, plot=True):\n",
    "    \"\"\"Calculate RMSE for each step of the trace to monitor convergence.\n",
    "    \"\"\"\n",
    "    burn_in = burn_in if len(pmf_model.trace) >= burn_in else 0\n",
    "    results = {'per-step-train': [], 'running-train': [],\n",
    "               'per-step-test': [], 'running-test': []}\n",
    "    R = np.zeros(test_data.shape)\n",
    "    for cnt, sample in enumerate(pmf_model.trace[burn_in:]):\n",
    "        sample_R = pmf_model.predict(sample['U'], sample['V'])\n",
    "        R += sample_R\n",
    "        running_R = R / (cnt + 1)\n",
    "        results['per-step-train'].append(rmse(train_data, sample_R))\n",
    "        results['running-train'].append(rmse(train_data, running_R))\n",
    "        results['per-step-test'].append(rmse(test_data, sample_R))\n",
    "        results['running-test'].append(rmse(test_data, running_R))\n",
    "\n",
    "    results = pd.DataFrame(results)\n",
    "\n",
    "    if plot:\n",
    "        results.plot(\n",
    "            kind='line', grid=False, figsize=(15, 7),\n",
    "            title='Per-step and Running RMSE From Posterior Predictive')\n",
    "\n",
    "    # Return the final predictions, and the RMSE calculations\n",
    "    return running_R, results\n",
    "\n",
    "\n",
    "PMF.running_rmse = _running_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted, results = pmf.running_rmse(test, train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And our final RMSE?\n",
    "final_test_rmse = results['running-test'].values[-1]\n",
    "final_train_rmse = results['running-train'].values[-1]\n",
    "print('Posterior predictive train RMSE: %.5f' % final_train_rmse)\n",
    "print('Posterior predictive test RMSE:  %.5f' % final_test_rmse)\n",
    "print('Train/test difference:           %.5f' % (final_test_rmse - final_train_rmse))\n",
    "print('Improvement from MAP:            %.5f' % (pmf_map_rmse - final_test_rmse))\n",
    "print('Improvement from Mean of Means:  %.5f' % (baselines['mom'] - final_test_rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 100  # RMSE doesn't really change after 100th sample anyway.\n",
    "all_results = pd.DataFrame({\n",
    "    'uniform random': np.repeat(baselines['ur'], size),\n",
    "    'global means': np.repeat(baselines['gm'], size),\n",
    "    'mean of means': np.repeat(baselines['mom'], size),\n",
    "    'PMF MAP': np.repeat(pmf_map_rmse, size),\n",
    "    'PMF MCMC': results['running-test'][:size],\n",
    "})\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "all_results.plot(kind='line', grid=False, ax=ax,\n",
    "                 title='RMSE for all methods')\n",
    "ax.set_xlabel(\"Number of Samples\")\n",
    "ax.set_ylabel(\"RMSE\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
